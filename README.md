# StudioOllamaUI

Portable graphical interface for Ollama, focused on privacy, ease of use, and local execution.

## Description
StudioOllamaUI is a fully portable application that allows interaction with LLM models through Ollama without complex installations. It is designed to run locally, preserve user privacy, and work from any folder or USB device.

## Features
- 100% portable application.
- No writes outside the root directory.
- Persistent chat history, models, and API keys.
- Support for local and cloud models.
- File system reading support.
- No Docker, Node.js, Python, Ollama serve or external installs required.
- Free and freely distributable.

## System requirements
- Windows 10 / 11 (64-bit).
- Minimum 4 GB RAM (quantized or cloud models).
- Automatic GPU usage if VRAM is available.
- At least 5 GB of free disk space.

## Getting started
1. Run `StudioOllamaUI.exe`.
2. Three terminal windows will open (do not close them).
3. Default model is `qwen2.5:0.5b`.
4. Manage models using the orange button.
5. Closing Firefox will stop all servers automatically.

## API Keys
- Ollama API (free): required for cloud models.
- Tavily API (free, recommended).
- Google / Bing APIs (paid).
- DuckDuckGo does not require an API key.

## Security
- Local execution only.
- Internet access only when web search is enabled.
- Does not execute commands automatically.
- No user data is collected.

## License
GPL-3.0

## Credits
Developer: Francesc Roig  
Contact: francesc@vivaldi.net  
GitHub: https://github.com/francescroig/StudioOllamaUI
Sourceforge: https://sourceforge.net/projects/studioollamaui/



# StudioOllamaUI

Interfaz gráfica portable para Ollama, enfocada en privacidad, facilidad de uso y ejecución local.

## Descripción
StudioOllamaUI es una aplicación totalmente portable que permite interactuar con modelos LLM mediante Ollama sin instalaciones complejas. Está pensada para ejecutarse de forma local, preservar la privacidad del usuario y funcionar desde cualquier carpeta o dispositivo USB.

## Características
- Aplicación 100 % portable.
- No escribe datos fuera de la carpeta raíz.
- Historial, modelos y claves API persistentes.
- Soporte para modelos locales y cloud.
- Lectura de archivos del sistema.
- No requiere Docker, Node.js, Python, Ollama server ni instalaciones externas.
- Gratuita y distribuible libremente.

## Requisitos del sistema
- Windows 10 / 11 (64 bits).
- Mínimo 4 GB de RAM (modelos cuantizados o cloud).
- Uso automático de GPU si hay VRAM disponible.
- Al menos 5 GB de espacio libre.

## Primeros pasos
1. Ejecuta `StudioOllamaUI.exe`.
2. Se abrirán tres ventanas de terminal (no cerrarlas).
3. El modelo por defecto es `qwen2.5:0.5b`.
4. Gestiona modelos desde el botón naranja.
5. Al cerrar Firefox, los servidores se detienen automáticamente.

## Claves API
- Ollama API (gratuita): modelos cloud.
- Tavily API (gratuita, recomendada).
- Google / Bing (de pago).
- DuckDuckGo no requiere clave.

## Seguridad
- Ejecución local.
- Acceso a internet solo si se activa búsqueda web.
- No ejecuta comandos automáticamente.
- No recopila datos del usuario.

## Licencia
GPL-3.0

## Créditos
Desarrollo: Francesc Roig  
Contacto: francesc@vivaldi.net  
GitHub: https://github.com/francescroig/StudioOllamaUI
Sourceforge: https://sourceforge.net/projects/studioollamaui/